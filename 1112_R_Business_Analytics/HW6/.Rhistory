point <- function() {
return (sum(sample(1:6,2)))
}
point()
if_win <- function() {
first_point <- point()
if (first_point == 7 || first_point == 11) {
return (TRUE)
} else if (first_point == 2 || first_point == 3 || first_point == 12) {
return (FALSE)
} else {
target <- first_point
retry_point <- 0
while (retry_point != 7 && retry_point != target) {
retry_point <- point()
}
if (retry_point == target) {
return (TRUE)
} else if (retry_point == 7) {
return (FALSE)
}
}
}
if_win()
expected_value(0.5, 10)
expected_value <- function(probability_of_winning, bet_size) {
return (probability_of_winning*bet_size - (1-probability_of_winning)*bet_size)
}
expected_value(0.5, 10)
expected_value <- function(probability_of_winning, bet_size) {
return (probability_of_winning*bet_size - (1-probability_of_winning)*bet_size)
}
expected_value(0.5, 10)
expected_value <- function(probability_of_winning, bet_size) {
return (probability_of_winning*bet_size - (1-probability_of_winning)*bet_size)
}
expected_value(0.6, 10)
# 1.最後需交出一個 function 叫 craps_game ，
#   該function有兩個輸入：num_simulations (the number of simulations to run)
#   和 bet_size (the size of the bet for each game)。function裡為模擬此賭博遊戲的輸贏。
# craps_game <- function(num_simulations, bet_size) { }
# 2.此賭博遊戲為，用sample()去模擬擲兩個骰子。加總擲出的兩個骰子數字，為點數和。
point <- function() {
return (sum(sample(1:6,2)))
}
point()
# 3.如果第一次點數和為 7 或 11，玩家勝，贏得 bet size 。
#   若點數和為 2、3 或 12，玩家輸，輸了 -1 x bet size。
#   如果點數和為其他點數，記錄第一次的點數和，然後繼續擲骰，
#   直至點數和等於第一次擲出的點數和，玩家勝，贏得 bet size。
#   若在這之前擲出了點數和為 7，玩家輸，輸了 -1 x bet size。
if_win <- function() {
first_point <- point()
if (first_point == 7 || first_point == 11) {
return (TRUE)
} else if (first_point == 2 || first_point == 3 || first_point == 12) {
return (FALSE)
} else {
target <- first_point
retry_point <- 0
while (retry_point != 7 && retry_point != target) {
retry_point <- point()
}
if (retry_point == target) {
return (TRUE)
} else if (retry_point == 7) {
return (FALSE)
}
}
}
if_win()
# 4.計算遊戲的期望值
#   expected value = (probability of winning x bet size) + (probability of losing x bet size).
expected_value <- function(probability_of_winning, bet_size) {
return (probability_of_winning*bet_size - (1-probability_of_winning)*bet_size)
}
expected_value(0.6, 10)
# 5.craps_game function 輸出（Return） 遊戲的期望值。
# 提示：先寫玩一次的function，贏為TRUE，輸為FALSE，輸出該結果。
#       再放進craps_game重複多次後，計算輸贏次數，獲得輸贏機率。
craps_game <- function(num_simulations, bet_size) {
win_count <- 0
for (i in c(1:num_simulations)) {
if (if_win()) {
win_count <- win_count + 1
}
}
probability_of_winning <- win_count/num_simulations
return(expected_value(probability_of_winning, bet_size))
}
craps_game(1000, 10)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data <- read.csv("ecommerce.csv")
data <- na.omit(data)
# Check your data1
str(data)
# Convert date and time string to timestamp
data$timestamp <- as.POSIXct(data$timestamp, format = "%M:%S")
data
# Descriptive statistics
summary(data)
# Test difference between groups
data %>%
group_by(group) %>%
summarise(mean_purchase = mean(converted))
# Test difference by country
data %>%
group_by(country) %>%
summarise(mean_purchase = mean(converted))
# Hypothesis Test:
# t-test for two sample mean
# Ha: mu_1 - mu_0 > 0
t.test(data[data$group == "treatment", ]$converted,
data[data$group == "control", ]$converted,
alternative = "greater")
# ANOVA
aov.model <- aov(
converted ~ group + country + landing_page, data)
summary(aov.model)
# Check the interaction term
interaction.model <- aov(converted ~ group * country, data)
summary(interaction.model)
# Post-hoc test
TukeyHSD(aov.model, "group")
TukeyHSD(aov.model, "country")
plot(TukeyHSD(aov.model, "country"))
# Check daily difference
daily.purchase <- data %>%
group_by(timestamp, group) %>%
summarise(purchase_amount = mean(converted))
ggplot(daily.purchase, aes(x = timestamp, y = purchase_amount, colour = as.factor(group))) +
geom_point() + geom_line() +
xlab("Date") + ylab("Purchase Amount") +
ggtitle("Time Series Plot of Purchase Amount: Test versus Control") +
theme_bw()
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data <- read.csv("ecommerce.csv")
data <- na.omit(data)
# Check your data1
str(data)
# Convert date and time string to timestamp
data$timestamp <- as.POSIXct(data$timestamp, format = "%M:%S")
data$minute <- minute(data$timestamp)
data
# Descriptive statistics
summary(data)
# Test difference between groups
data %>%
group_by(group) %>%
summarise(mean_purchase = mean(converted))
# Test difference by country
data %>%
group_by(country) %>%
summarise(mean_purchase = mean(converted))
# Hypothesis Test:
# t-test for two sample mean
# Ha: mu_1 - mu_0 > 0
t.test(data[data$group == "treatment", ]$converted,
data[data$group == "control", ]$converted,
alternative = "greater")
# ANOVA
aov.model <- aov(
converted ~ group + country + landing_page, data)
summary(aov.model)
# Check the interaction term
interaction.model <- aov(converted ~ group * country, data)
summary(interaction.model)
# Post-hoc test
TukeyHSD(aov.model, "group")
TukeyHSD(aov.model, "country")
plot(TukeyHSD(aov.model, "country"))
# Check daily difference
daily.purchase <- data %>%
group_by(minute, group) %>%
summarise(purchase_amount = mean(converted))
ggplot(daily.purchase, aes(x = minute, y = purchase_amount, colour = as.factor(group))) +
geom_point() + geom_line() +
xlab("Date") + ylab("Purchase Amount") +
ggtitle("Time Series Plot of Purchase Amount: Test versus Control") +
theme_bw()
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data <- read.csv("ecommerce.csv")
data <- na.omit(data)
# Check your data1
str(data)
# Convert date and time string to timestamp
data$timestamp <- as.POSIXct(data$timestamp, format = "%M:%S")
data
# Descriptive statistics
summary(data)
# Test difference between groups
data %>%
group_by(group) %>%
summarise(mean_purchase = mean(converted))
# Test difference by country
data %>%
group_by(country) %>%
summarise(mean_purchase = mean(converted))
# Hypothesis Test:
# t-test for two sample mean
# Ha: mu_1 - mu_0 > 0
t.test(data[data$group == "treatment", ]$converted,
data[data$group == "control", ]$converted,
alternative = "greater")
# ANOVA
aov.model <- aov(
converted ~ group + country + landing_page, data)
summary(aov.model)
# Check the interaction term
interaction.model <- aov(converted ~ group * country, data)
summary(interaction.model)
# Post-hoc test
TukeyHSD(aov.model, "group")
TukeyHSD(aov.model, "country")
plot(TukeyHSD(aov.model, "country"))
# Check daily difference
daily.purchase <- data %>%
group_by(timestamp, group) %>%
summarise(purchase_amount = mean(converted))
ggplot(daily.purchase, aes(x = timestamp, y = purchase_amount, colour = as.factor(group))) +
geom_point() + geom_line() +
xlab("Date") + ylab("Purchase Amount") +
ggtitle("Time Series Plot of Purchase Amount: Test versus Control") +
theme_bw()
ggplot(data, aes(x = country, y = converted, colour = test)) +
geom_boxplot() +
xlab("Country") + ylab("Purchase Amount") +
ggtitle("Boxplot of Purchase Amount by Country: Test versus Control") +
theme_bw()
ggplot(data, aes(x = country, y = converted, colour = group)) +
geom_boxplot() +
xlab("Country") + ylab("Purchase Amount") +
ggtitle("Boxplot of Purchase Amount by Country: Test versus Control") +
theme_bw()
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>% filter(website == "A" & converted == 1)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>% filter(website == "old_page" & converted == 1)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>% filter(landing_page == "old_page" & converted == 1)
purchased_A <- nrow(subset_A)
visitors_A <- nrow(data2 %>% filter(landing_page == "old_page"))
phat_A <- purchased_A / visitors_A
# Website B: proportion of converted
subset_B <- data2 %>% filter(landing_page == "new_page" & converted == 1)
purchased_B <- nrow(subset_B)
visitors_B <- nrow(data2 %>% filter(landing_page == "new_page"))
phat_B <- purchased_B / visitors_B
# Uplift calculation
uplift <- (phat_B - phat_A) / phat_A * 100
# Pooled proportion
p_pool <- (purchased_A + purchased_B) / (visitors_A + visitors_B)
# Standard error of the pooled proportion
SE_pool <- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) + (1 / visitors_B)))
# Point Estimate or Difference in proportion
d_hat <- phat_B - phat_A
# Z-score
z_score <- d_hat / SE_pool
# Two-sided p-value
p_value <- pnorm(q = -abs(z_score), mean = 0, sd = 1) * 2
# Confidence interval
ci <- c(d_hat - qnorm(0.975) * SE_pool, d_hat + qnorm(0.975) * SE_pool)
# SE and CI for website A and B separately
se_hat_A <- sqrt(phat_A * (1 - phat_A) / visitors_A)
ci_A <- c(phat_A - qnorm(0.975) * se_hat_A, phat_A + qnorm(0.975) * se_hat_A)
se_hat_B <- sqrt(phat_B * (1 - phat_B) /
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>% filter(landing_page == "old_page" & converted == 1)
purchased_A <- nrow(subset_A)
visitors_A <- nrow(data2 %>% filter(landing_page == "old_page"))
phat_A <- purchased_A / visitors_A
# Website B: proportion of converted
subset_B <- data2 %>% filter(landing_page == "new_page" & converted == 1)
purchased_B <- nrow(subset_B)
visitors_B <- nrow(data2 %>% filter(landing_page == "new_page"))
phat_B <- purchased_B / visitors_B
# Uplift calculation
uplift <- (phat_B - phat_A) / phat_A * 100
# Pooled proportion
p_pool <- (purchased_A + purchased_B) / (visitors_A + visitors_B)
# Standard error of the pooled proportion
SE_pool <- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) + (1 / visitors_B)))
# Point Estimate or Difference in proportion
d_hat <- phat_B - phat_A
# Z-score
z_score <- d_hat / SE_pool
# Two-sided p-value
p_value <- pnorm(q = -abs(z_score), mean = 0, sd = 1) * 2
# Confidence interval
ci <- c(d_hat - qnorm(0.975) * SE_pool, d_hat + qnorm(0.975) * SE_pool)
# SE and CI for website A and B separately
se_hat_A <- sqrt(phat_A * (1 - phat_A) / visitors_A)
ci_A <- c(phat_A - qnorm(0.975) * se_hat_A, phat_A + qnorm(0.975) * se_hat_A)
se_hat_B <- sqrt(phat_B * (1 - phat_B) / visitors_B)
ci_B <- c(phat_B - qnorm(0.975) * se_hat_B, phat_B + qnorm(0.975) * se_hat_B)
# 1-sample test
prop.test(c(purchased_A + purchased_B), c(visitors_A + visitors_B))
# 2-sample test
prop.test(c(purchased_A, purchased_B), c(visitors_A, visitors_B))
# Chi-squared test
chisq.test(data2$converted, data2$website)
# Fake data
x <- seq(from = 90, by = 10, length.out = 6)
n <- rep(1000, 6)
prop.test(x, n) # goodness-of-fit test
pairwise.prop.test(x, n, p.adjust.method = "none")
# Sample size determination
pwr.anova.test(k = 2, n = NULL, f = 0.2, sig.level = 0.05, power = 0.8)
pwr.anova.test(k = 2, n = NULL, f = 0.2, sig.level = 0.05, power = 0.8)
install.packages("pwr")
library(pwr)
# install.packages("pwr")
library(pwr)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>% filter(landing_page == "old_page" & converted == 1)
purchased_A <- nrow(subset_A)
visitors_A <- nrow(data2 %>% filter(landing_page == "old_page"))
phat_A <- purchased_A / visitors_A
# Website B: proportion of converted
subset_B <- data2 %>% filter(landing_page == "new_page" & converted == 1)
purchased_B <- nrow(subset_B)
visitors_B <- nrow(data2 %>% filter(landing_page == "new_page"))
phat_B <- purchased_B / visitors_B
# Uplift calculation
uplift <- (phat_B - phat_A) / phat_A * 100
# Pooled proportion
p_pool <- (purchased_A + purchased_B) / (visitors_A + visitors_B)
# Standard error of the pooled proportion
SE_pool <- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) + (1 / visitors_B)))
# Point Estimate or Difference in proportion
d_hat <- phat_B - phat_A
# Z-score
z_score <- d_hat / SE_pool
# Two-sided p-value
p_value <- pnorm(q = -abs(z_score), mean = 0, sd = 1) * 2
# Confidence interval
ci <- c(d_hat - qnorm(0.975) * SE_pool, d_hat + qnorm(0.975) * SE_pool)
# SE and CI for website A and B separately
se_hat_A <- sqrt(phat_A * (1 - phat_A) / visitors_A)
ci_A <- c(phat_A - qnorm(0.975) * se_hat_A, phat_A + qnorm(0.975) * se_hat_A)
se_hat_B <- sqrt(phat_B * (1 - phat_B) / visitors_B)
ci_B <- c(phat_B - qnorm(0.975) * se_hat_B, phat_B + qnorm(0.975) * se_hat_B)
# 1-sample test
prop.test(c(purchased_A + purchased_B), c(visitors_A + visitors_B))
# 2-sample test
prop.test(c(purchased_A, purchased_B), c(visitors_A, visitors_B))
# Chi-squared test
chisq.test(data2$converted, data2$website)
# Fake data
x <- seq(from = 90, by = 10, length.out = 6)
n <- rep(1000, 6)
prop.test(x, n) # goodness-of-fit test
pairwise.prop.test(x, n, p.adjust.method = "none")
# Sample size determination
pwr.anova.test(k = 2, n = NULL, f = 0.2, sig.level = 0.05, power = 0.8)
pwr.t.test(n = NULL, d = 0.3, sig.level = 0.05, power = 0.8, type = "two.sample", alternative = "greater")
# install.packages("pwr")
library(pwr)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>% filter(landing_page == "old_page" & converted == 1)
subset_A <- data2 %>% filter(landing_page == "old_page" & converted == 1)
# Website A: proportion of converted
subset_A <- data2 %>%
filter(landing_page == "old_page" & converted == 1)
# install.packages("pwr")
library(pwr)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
data2 %>%
filter(landing_page == "old_page" & converted == 1)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
data2 <- na.omit(data)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
# Remove missing values
data2 <- na.omit(data2)
# install.packages("pwr")
library(pwr)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
# Remove missing values
data2 <- na.omit(data2)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>%
filter(landing_page == "old_page" & converted == 1)
purchased_A <- nrow(subset_A)
# install.packages("pwr")
library(pwr)
library(tidyverse)
setwd("C:\\Users\\ASUS\\Desktop\\五234 R\\HW6") #放你的路徑
data2 <- read.csv("ecommerce.csv")
# Remove missing values
data2 <- na.omit(data2)
# Data2
# Website A: proportion of converted
subset_A <- data2 %>%
filter(landing_page == "old_page" & converted == 1)
purchased_A <- nrow(subset_A)
visitors_A <- nrow(data2 %>% filter(landing_page == "old_page"))
phat_A <- purchased_A / visitors_A
# Website B: proportion of converted
subset_B <- data2 %>% filter(landing_page == "new_page" & converted == 1)
purchased_B <- nrow(subset_B)
visitors_B <- nrow(data2 %>% filter(landing_page == "new_page"))
phat_B <- purchased_B / visitors_B
# Uplift calculation
uplift <- (phat_B - phat_A) / phat_A * 100
# Pooled proportion
p_pool <- (purchased_A + purchased_B) / (visitors_A + visitors_B)
# Standard error of the pooled proportion
SE_pool <- sqrt(p_pool * (1 - p_pool) * ((1 / visitors_A) + (1 / visitors_B)))
# Point Estimate or Difference in proportion
d_hat <- phat_B - phat_A
# Z-score
z_score <- d_hat / SE_pool
# Two-sided p-value
p_value <- pnorm(q = -abs(z_score), mean = 0, sd = 1) * 2
# Confidence interval
ci <- c(d_hat - qnorm(0.975) * SE_pool, d_hat + qnorm(0.975) * SE_pool)
# SE and CI for website A and B separately
se_hat_A <- sqrt(phat_A * (1 - phat_A) / visitors_A)
ci_A <- c(phat_A - qnorm(0.975) * se_hat_A, phat_A + qnorm(0.975) * se_hat_A)
se_hat_B <- sqrt(phat_B * (1 - phat_B) / visitors_B)
ci_B <- c(phat_B - qnorm(0.975) * se_hat_B, phat_B + qnorm(0.975) * se_hat_B)
# 1-sample test
prop.test(c(purchased_A + purchased_B), c(visitors_A + visitors_B))
# 2-sample test
prop.test(c(purchased_A, purchased_B), c(visitors_A, visitors_B))
# Chi-squared test
chisq.test(data2$converted, data2$website)
# Fake data
x <- seq(from = 90, by = 10, length.out = 6)
n <- rep(1000, 6)
prop.test(x, n) # goodness-of-fit test
pairwise.prop.test(x, n, p.adjust.method = "none")
# Sample size determination
pwr.anova.test(k = 2, n = NULL, f = 0.2, sig.level = 0.05, power = 0.8)
pwr.t.test(n = NULL, d = 0.3, sig.level = 0.05, power = 0.8, type = "two.sample", alternative = "greater")
